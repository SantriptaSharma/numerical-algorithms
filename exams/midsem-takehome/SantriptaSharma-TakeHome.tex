\documentclass[12pt]{article}
\usepackage[a4paper, total={6.5in, 10in}]{geometry}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

\newcommand{\code}{CS-2363}
\newcommand{\name}{Midsem Exam Take Home}
\newcommand{\me}{Santripta Sharma}
\newcommand{\prob}[1]{\mathbb{P}\left[#1\right]}

\newcommand{\rec}[1]{\frac{1}{#1}}

\newenvironment*{qparts}{\begin{enumerate}[label=(\alph*)]}{\end{enumerate}}

\title{\code: \name}
\author{\me}
\date{\today}

\markright{\code\hspace{0.5in}\name\hspace{0.5in}\me}

\parindent=0pt

\begin{document}
\maketitle
\hrule \bigskip

Consider the problem of drawing a map of $n$ cities given the pair-wise distances between them.\medskip

More formally, we have $n$ cities, an array $D$ of the $n^2$ distances, $D_{ij} = \text{dist}(\text{city}_i, \text{city}_j)$ ($D_{ij} = D_{ji}$ with ordinary luck!), and we want to know if there are points $P_i$ in the plane  such that $\forall i, j, \text{dist}(P_i, P_j) = D_{ij}$. We may also wish to find an appropriate "approximate solution" if such $P_i$s do not exist (perhaps by fudging it a little).\medskip

Let us first see if there are any such points in $\mathbb{R}^n$. If there are, we may shift the lot so that the center of mass is at the origin. Then, the distances can be related to the inner products in the following way. Assuming $\sum_i P_i = 0$, you can show that the inner products are $P_i^tP_j = A_{ij}$, where

\begin{align*}
	S_i = \frac{1}{n}\sum^n_{j=1}D_{ij}^2,&\ &T=\frac{1}{n^2}\sum^n_{i,j=1}D_{ij}^2,&\ &A_{ij} = -\frac{1}{2}(D_{ij}^2 - S_i - S_j + T)
\end{align*}

Thus, if such $P_i \in \mathbb{R}^n$ can be found whose inner products are $A_{ij}$, we can readily compute that $\text{dist}(P_i, P_j) = D_{ij}$. Also, if $X$ is the matrix with $P_i$s as its columns, the problem reduces to finding an $X$ such that $A = X^tX$. Clearly, in such a case, $A$ must be symmetric and positive definite, since if $v$ is any vector in $\mathbb{R}^n$ then $v^t A v = (Xv)^t(Xv) \geq 0$. Is this related to the triangle inequality?\medskip

So, now our (actually your) problem is as follows:

Given a symmetric matrix $A$, {\bf (a)} how do we decide if there are $n$-dimensional vectors whose inner products are the entries of $A$? {\bf (b)} If the solution exists, is it unique, possibly up to some normalisations? {\bf (c)} How do we know if they lie in a plane? {\bf (d)} How do we find them if they exist? and {\bf (e)} fudge if they do not?\bigskip

\begin{qparts}
	\item We will show that $A$ being positive semi-definite is a necessary and sufficient condition for the existence of vectors $X$ such that $A = X^tX$.\bigskip
	
	$\Leftarrow$:

	$\exists X, A = X^tX \Rightarrow \forall v, v^tAv = (v^tX^t)Xv = (Xv)^t(Xv) = \lVert Xv \rVert^2 \geq 0$. QED.\bigskip

	$\Rightarrow$:

	Let $A$ be positive semi-definite. Then, by the spectral theorem for symmetric matrices, $A = U\Lambda U^t$ for some orthogonal matrix $U$ and diagonal matrix $\Lambda$ with non-negative entries (positive semi-definite matrices have positive eigenvalues). That is, $\Lambda = \text{diag}(\lambda_1, \lambda_2, \lambda_3, \dots, \lambda_n)$, each of them non-negative and real. Then, consider $D = \text{diag}(\sqrt{\lambda_1}, \sqrt{\lambda_2}, \dots, \sqrt{\lambda_n}), D^tD = DD^t = \Lambda$. \medskip

	Write $X = DU^t$. Then, $X^tX = UDDU^t = U\Lambda U^t = A$. QED.

	\item The solution is not unique. Given any $X = [P_1\ P_2\ P_3\ \dots\ P_n]$ satisfying our condition, we can multiply each $P_i$ by any orthogonal matrix (a rotation) and the dot products will not change (since the inner product of any two vectors is invariant under rotation, as neither the relative angles nor the magnitudes change).\medskip
	
	Further, we can flip/mirror the vectors around any of the axes and the inner  products will still remain the same (since the sign flip occurs on both vectors, by linearity \& symmetry of the real inner product, we can remove $-1 \times -1 = 1$ from the inner product in all flipped components).\medskip

	Thus, the solution is not unique, but unique up to rotations and flips.

	\item In order for the vectors to lie in a plane, we want $X$ to have a rank of at most $2$, since this implies that all the vectors can be generated by a linear combination of some two vectors. It is easy to see that this is a necessary and sufficient condition.\medskip
	
	Notice that $\text{Rank } X = \text{Rank } A = r$ since $X = DU^t$, where $D$ is a diagonal matrix with exactly $r$ non-zero entries (a square rank $r$ matrix has exactly $r$ non-zero singular/eigen values), so each vector in $X$ has, at best, only the first $r$ components non-zero which means the elementary basis of size $r$ will span it.\medskip

	Then, there exist a planar set of such vectors if and only if $\text{Rank } X = \text{Rank } A \leq 2$.

	\item To find the vectors, as mentioned before, we simply diagonalise $A = U\Lambda U^t$ (positive semi-definite + symmetric guarantees diagonalizability). Then, the vectors that are columns of $X = DU^t$ will satisfy the condition.\medskip
	
	Computationally, we can use the Rayleigh-Quotient power iteration to find the required eigenvectors $U$ and eigenvalues $\Lambda = D^tD$. Since we only need the first two eigenvectors and eigenvalues (since Rank $A = 2$ in this existence case), this converges quickly.

	\item If the vectors do not exist, i.e. $X$ has more than two non-zero rows, we claim that the best approximation to the given $A$ is given by taking the best rank $2$ approximation to $A$. This can be computed by taking $X$ and zeroing out all the vector components after the first two, i.e. zeroing out/dropping all rows of $X$ past the first two.\medskip
	
	We've already proven why the truncated SVD is the best rank $k$ approximation to a matrix, and this is a special case of that (where the SVD = spectral decomposition).\medskip
\end{qparts}

\end{document}